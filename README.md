# Autonomous-Driving-Foundation-Stack-Student-Edition-
This project demonstrates an end-to-end understanding of how a modern self-driving system works â€” starting from raw camera video and ending with a refined driving policy.
# Autonomous Driving Foundation Stack (Student Edition)

A **camera-only autonomous driving pipeline** showcasing multi-task perception, end-to-end imitation learning, offline reinforcement learning, and a lightweight data engine.  

---

## ğŸš— Project Summary
This project demonstrates an end-to-end understanding of how a modern self-driving system works â€” starting from raw camera video and ending with a refined driving policy.

It includes:
- **Multi-task video perception** (lane segmentation + steering prediction)
- **End-to-end imitation learning** on real driving data
- **Offline reinforcement learning** to make the policy safer and more stable
- **A lightweight data engine** for filtering and improving training data
- **Optional generative augmentation** for rare driving scenarios

The goal is not to build a production system, but to create a **clear, research-style, educational autonomy stack**.

---

## ğŸ§© Core Modules
### 1) Multi-Task Video Perception (`perception/`)
A compact video backbone (3D CNN or TimeSformer-lite) jointly predicts:
- Lane segmentation
- Steering angle

This module demonstrates **multi-task learning**, **video modeling**, and **shared encoder design**.

### 2) End-to-End Imitation Learning (`imitation_learning/`)
Uses real driving data (e.g., comma.ai) to learn human driving behavior:
- **Input:** short video clip
- **Output:** steering angle (and optionally throttle/brake)

Implements temporal modeling via **LSTM** or **3D CNN**.

### 3) Offline Reinforcement Learning (`offline_rl/`)
Refines the imitation policy with offline RL:
- Behavior Cloning (baseline)
- Conservative Q-Learning (CQL) or Implicit Q-Learning (IQL)

Focus: policy stability, safety, reduced interventions.

### 4) Data Engine + Synthetic Data (`data_engine/`)
A small but powerful data-centric module:
- Confidence-based sample filtering
- Identification of poor/ambiguous frames
- Optional diffusion-based synthetic data generation for rare scenarios

This simulates how real autonomy teams maintain **high-quality datasets**.

---

## ğŸ—ï¸ Architecture Diagram
```
Raw Driving Video â†’ Data Engine â†’ Perception Model â†’ Imitation Policy â†’ Offline RL Policy â†’ Simulator Demo
```

Detailed view:
```
                +---------------------------+
                |    Raw Driving Dataset    |
                +-------------+-------------+
                              |
                              v
                   +-------------------+
                   |     Data Engine   |
                   +---------+---------+
                             |
                +------------+------------+
                |                         |
                v                         v
      +--------------------+    +----------------------+
      | Perception Model   |    | Imitation Policy     |
      +---------+----------+    +----------+-----------+
                |                          |
                +------------+-------------+
                             v
                   +----------------------+
                   |   Offline RL Policy  |
                   +----------+-----------+
                              |
                              v
                      +---------------+
                      |   Simulator   |
                      +---------------+
```

---

## ğŸ”§ Tech Stack
- **Python**, PyTorch
- 3D CNN or lightweight TimeSformer
- OpenCV, Albumentations
- comma.ai dataset (or similar)
- Stable-Baselines3 / custom RL
- CARLA simulator for evaluation (optional)

---

## ğŸ“ Repository Structure
```
autonomous-driving-foundation-stack/
â”œâ”€â”€ README.md
â”œâ”€â”€ perception/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ train_perception.py
â”œâ”€â”€ imitation_learning/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ train_imitation.py
â”œâ”€â”€ offline_rl/
â”‚   â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ train_rl.py
â”œâ”€â”€ data_engine/
â”‚   â”œâ”€â”€ filtering/
â”‚   â”œâ”€â”€ synthetic/
â”œâ”€â”€ scripts/
â”œâ”€â”€ configs/
â”œâ”€â”€ notebooks/
â””â”€â”€ .gitignore
```

---

## ğŸš€ Getting Started
```
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
Download a driving dataset (comma.ai or open-source datasets) and update paths inside `configs/*.yaml`.

---

## ğŸ§ª Training
### Perception
```
python perception/train_perception.py --config configs/perception.yaml
```

### Imitation Learning
```
python imitation_learning/train_imitation.py --config configs/imitation.yaml
```

### Offline RL
```
python offline_rl/train_rl.py --config configs/offline_rl.yaml
```

---

## ğŸ“Š Evaluation
- **Perception:** IoU, segmentation overlays
- **Imitation:** MSE/MAE for steering prediction
- **Offline RL:** lane departures, safety score, intervention count in simulator

---

## ğŸ§­ Roadmap
- [x] Baseline imitation learning
- [x] Basic perception model
- [ ] Unified multi-task video backbone
- [ ] Offline RL integration
- [ ] Data engine filtering
- [ ] Synthetic rare-case augmentation
- [ ] Simulator demo and analysis

---

## ğŸ¯ Project Motivation
This project brings together the core components of modern autonomous driving systems in a compact, research-focused format:
- Video foundation models
- Multi-task perception
- Imitation learning
- Offline reinforcement learning
- Data-centric pipelines


---

## ğŸ“¬ Contact
For collaboration or feedback, feel free to reach out.
